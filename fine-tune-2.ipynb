{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d1049cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tensorflow in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (2.19.0)\n",
      "Collecting datasets\n",
      "  Using cached datasets-3.6.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: transformers in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (4.52.3)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (2.2.2)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=24.3.25 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (25.2.10)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (0.6.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (0.2.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (18.1.1)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (3.4.0)\n",
      "Requirement already satisfied: packaging in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (25.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (5.29.4)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (2.32.3)\n",
      "Requirement already satisfied: setuptools in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (80.8.0)\n",
      "Requirement already satisfied: six>=1.12.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (1.17.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (3.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (4.12.2)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (1.17.2)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (1.71.0)\n",
      "Requirement already satisfied: tensorboard~=2.19.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (2.19.0)\n",
      "Requirement already satisfied: keras>=3.5.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (3.10.0)\n",
      "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (2.1.2)\n",
      "Requirement already satisfied: h5py>=3.11.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (3.13.0)\n",
      "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (0.5.1)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorflow) (0.31.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
      "Requirement already satisfied: markdown>=2.6.8 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.8)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
      "Requirement already satisfied: filelock in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from datasets) (3.13.1)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Using cached pyarrow-20.0.0-cp311-cp311-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Using cached dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Using cached xxhash-3.5.0-cp311-cp311-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Using cached multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n",
      "Requirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from datasets) (0.32.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Downloading aiohttp-3.12.4-cp311-cp311-win_amd64.whl.metadata (7.9 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from transformers) (2024.11.6)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from transformers) (0.21.1)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from transformers) (0.5.3)\n",
      "Collecting aiohappyeyeballs>=2.5.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached frozenlist-1.6.0-cp311-cp311-win_amd64.whl.metadata (16 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached multidict-6.4.4-cp311-cp311-win_amd64.whl.metadata (5.5 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached propcache-0.3.1-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n",
      "  Using cached yarl-1.20.0-cp311-cp311-win_amd64.whl.metadata (74 kB)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
      "Requirement already satisfied: rich in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (14.0.0)\n",
      "Requirement already satisfied: namex in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
      "Requirement already satisfied: optree in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
      "Requirement already satisfied: colorama in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
      "Requirement already satisfied: mdurl~=0.1 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
      "Using cached datasets-3.6.0-py3-none-any.whl (491 kB)\n",
      "Using cached dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Using cached multiprocess-0.70.16-py311-none-any.whl (143 kB)\n",
      "Downloading aiohttp-3.12.4-cp311-cp311-win_amd64.whl (444 kB)\n",
      "Using cached multidict-6.4.4-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Using cached yarl-1.20.0-cp311-cp311-win_amd64.whl (93 kB)\n",
      "Using cached aiohappyeyeballs-2.6.1-py3-none-any.whl (15 kB)\n",
      "Using cached aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached frozenlist-1.6.0-cp311-cp311-win_amd64.whl (120 kB)\n",
      "Using cached propcache-0.3.1-cp311-cp311-win_amd64.whl (45 kB)\n",
      "Using cached pyarrow-20.0.0-cp311-cp311-win_amd64.whl (25.8 MB)\n",
      "Using cached xxhash-3.5.0-cp311-cp311-win_amd64.whl (30 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, frozenlist, dill, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "\n",
      "   --- ------------------------------------  1/12 [pyarrow]\n",
      "   --- ------------------------------------  1/12 [pyarrow]\n",
      "   --- ------------------------------------  1/12 [pyarrow]\n",
      "   --- ------------------------------------  1/12 [pyarrow]\n",
      "   --- ------------------------------------  1/12 [pyarrow]\n",
      "   --- ------------------------------------  1/12 [pyarrow]\n",
      "   --- ------------------------------------  1/12 [pyarrow]\n",
      "   --- ------------------------------------  1/12 [pyarrow]\n",
      "   --- ------------------------------------  1/12 [pyarrow]\n",
      "   --- ------------------------------------  1/12 [pyarrow]\n",
      "   --- ------------------------------------  1/12 [pyarrow]\n",
      "   --- ------------------------------------  1/12 [pyarrow]\n",
      "   --- ------------------------------------  1/12 [pyarrow]\n",
      "   --- ------------------------------------  1/12 [pyarrow]\n",
      "   --- ------------------------------------  1/12 [pyarrow]\n",
      "   --- ------------------------------------  1/12 [pyarrow]\n",
      "   ------ ---------------------------------  2/12 [propcache]\n",
      "   ------------- --------------------------  4/12 [frozenlist]\n",
      "   ---------------- -----------------------  5/12 [dill]\n",
      "   ---------------- -----------------------  5/12 [dill]\n",
      "   ---------------- -----------------------  5/12 [dill]\n",
      "   ---------------- -----------------------  5/12 [dill]\n",
      "   -------------------- -------------------  6/12 [aiohappyeyeballs]\n",
      "   ----------------------- ----------------  7/12 [yarl]\n",
      "   -------------------------- -------------  8/12 [multiprocess]\n",
      "   -------------------------- -------------  8/12 [multiprocess]\n",
      "   -------------------------- -------------  8/12 [multiprocess]\n",
      "   -------------------------- -------------  8/12 [multiprocess]\n",
      "   ------------------------------ ---------  9/12 [aiosignal]\n",
      "   ------------------------------ ---------  9/12 [aiosignal]\n",
      "   --------------------------------- ------ 10/12 [aiohttp]\n",
      "   --------------------------------- ------ 10/12 [aiohttp]\n",
      "   --------------------------------- ------ 10/12 [aiohttp]\n",
      "   --------------------------------- ------ 10/12 [aiohttp]\n",
      "   --------------------------------- ------ 10/12 [aiohttp]\n",
      "   --------------------------------- ------ 10/12 [aiohttp]\n",
      "   ------------------------------------ --- 11/12 [datasets]\n",
      "   ------------------------------------ --- 11/12 [datasets]\n",
      "   ------------------------------------ --- 11/12 [datasets]\n",
      "   ------------------------------------ --- 11/12 [datasets]\n",
      "   ------------------------------------ --- 11/12 [datasets]\n",
      "   ------------------------------------ --- 11/12 [datasets]\n",
      "   ------------------------------------ --- 11/12 [datasets]\n",
      "   ------------------------------------ --- 11/12 [datasets]\n",
      "   ------------------------------------ --- 11/12 [datasets]\n",
      "   ------------------------------------ --- 11/12 [datasets]\n",
      "   ---------------------------------------- 12/12 [datasets]\n",
      "\n",
      "Successfully installed aiohappyeyeballs-2.6.1 aiohttp-3.12.4 aiosignal-1.3.2 datasets-3.6.0 dill-0.3.8 frozenlist-1.6.0 multidict-6.4.4 multiprocess-0.70.16 propcache-0.3.1 pyarrow-20.0.0 xxhash-3.5.0 yarl-1.20.0\n"
     ]
    }
   ],
   "source": [
    "!pip install tensorflow datasets transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "14adccb3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\Diagnoses Prediction\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFAutoModel, AutoTokenizer, pipeline\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30130beb",
   "metadata": {},
   "source": [
    "Check CUDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "714b7004",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: True\n",
      "Device: NVIDIA GeForce GTX 1080\n",
      "2.6.0+cu126\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"Device:\", torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA not available â€” still CPU only\")\n",
    "\n",
    "print(torch.__version__)  # should be 2.6.0 or newer\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6e8d302",
   "metadata": {},
   "source": [
    "Load QA Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79156737",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\Diagnoses Prediction\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:143: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Berat\\.cache\\huggingface\\hub\\datasets--nehal69--bioAsq_Extractive_QA. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 3266 examples [00:00, 15650.08 examples/s]\n"
     ]
    }
   ],
   "source": [
    "bio_asq = load_dataset(\"nehal69/bioAsq_Extractive_QA\", field =\"data\", split=\"train[:3000]\")\n",
    "bio_asq = bio_asq.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a511abca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'answers', 'context'],\n",
       "        num_rows: 2400\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'question', 'answers', 'context'],\n",
       "        num_rows: 600\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bio_asq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2f36c6",
   "metadata": {},
   "source": [
    "Convert to Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "90d2c180",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>question</th>\n",
       "      <th>answers</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5324bdba9b2d7acc7e00001a_003</td>\n",
       "      <td>How is bladder wall thickness measured?</td>\n",
       "      <td>[{'answer_start': 670, 'text': 'Ultrasound'}]</td>\n",
       "      <td>Ultrasound estimated bladder weight in asympto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>52ed795098d0239505000032_037</td>\n",
       "      <td>Is the transcriptional regulator BACH1 an acti...</td>\n",
       "      <td>[{'answer_start': 112, 'text': 'repressor'}]</td>\n",
       "      <td>Heme regulates gene expression by triggering C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5353aedb288f4dae47000006_015</td>\n",
       "      <td>Which is the transcript responsible for X-chro...</td>\n",
       "      <td>[{'answer_start': 708, 'text': 'Xist'}]</td>\n",
       "      <td>Histone acetylation controls the inactive X ch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>553c9f96f32186855800000c_006</td>\n",
       "      <td>How are ultraconserved elements called when th...</td>\n",
       "      <td>[{'answer_start': 488, 'text': 'gene regulator...</td>\n",
       "      <td>Genomic context analysis reveals dense interac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>55200c606b348bb82c000013_266</td>\n",
       "      <td>Which clotting factor is inhibited by betrixaban?</td>\n",
       "      <td>[{'answer_start': 37, 'text': 'Xa'}]</td>\n",
       "      <td>Evaluation of the oral direct factor Xa inhibi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id  \\\n",
       "0  5324bdba9b2d7acc7e00001a_003   \n",
       "1  52ed795098d0239505000032_037   \n",
       "2  5353aedb288f4dae47000006_015   \n",
       "3  553c9f96f32186855800000c_006   \n",
       "4  55200c606b348bb82c000013_266   \n",
       "\n",
       "                                            question  \\\n",
       "0            How is bladder wall thickness measured?   \n",
       "1  Is the transcriptional regulator BACH1 an acti...   \n",
       "2  Which is the transcript responsible for X-chro...   \n",
       "3  How are ultraconserved elements called when th...   \n",
       "4  Which clotting factor is inhibited by betrixaban?   \n",
       "\n",
       "                                             answers  \\\n",
       "0      [{'answer_start': 670, 'text': 'Ultrasound'}]   \n",
       "1       [{'answer_start': 112, 'text': 'repressor'}]   \n",
       "2            [{'answer_start': 708, 'text': 'Xist'}]   \n",
       "3  [{'answer_start': 488, 'text': 'gene regulator...   \n",
       "4               [{'answer_start': 37, 'text': 'Xa'}]   \n",
       "\n",
       "                                             context  \n",
       "0  Ultrasound estimated bladder weight in asympto...  \n",
       "1  Heme regulates gene expression by triggering C...  \n",
       "2  Histone acetylation controls the inactive X ch...  \n",
       "3  Genomic context analysis reveals dense interac...  \n",
       "4  Evaluation of the oral direct factor Xa inhibi...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd \n",
    "# Convert the dataset to a dictionary\n",
    "data_dict = bio_asq[\"train\"].to_dict()\n",
    "# Create a DataFrame from the dictionary\n",
    "df = pd.DataFrame.from_dict(data_dict)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8763f25e",
   "metadata": {},
   "source": [
    "List all questions and answers for inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "55b2b210",
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [q.strip() for q in df[\"question\"]]\n",
    "context = [q.strip() for q in df[\"context\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdf6b4d8",
   "metadata": {},
   "source": [
    "Initialize Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "248394a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "\n",
    "inputs = tokenizer(\n",
    "        questions,\n",
    "        context,\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e72638c",
   "metadata": {},
   "source": [
    "Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888fe937",
   "metadata": {},
   "outputs": [],
   "source": [
    "offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "answers = df['answers'] # Target label\n",
    "for i, offset in enumerate(offset_mapping):\n",
    "    answer = answers[i]\n",
    "    start_char = answer[0][\"answer_start\"]\n",
    "    end_char = answer[0][\"answer_start\"] + len(answer[0][\"text\"])\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    # Find the start and end of the context\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "        idx += 1\n",
    "    context_start = idx\n",
    "    while sequence_ids[idx] == 1:\n",
    "        idx += 1\n",
    "    context_end = idx - 1\n",
    "    \n",
    "    # If the answer is not fully inside the context, label it (0, 0)\n",
    "    if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "        start_positions.append(0)\n",
    "        end_positions.append(0)\n",
    "    else:\n",
    "        # Otherwise it's the start and end token positions\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset[idx][0] <= start_char:\n",
    "            idx += 1\n",
    "        start_positions.append(idx - 1)\n",
    "\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset[idx][1] >= end_char:\n",
    "            idx -= 1\n",
    "        end_positions.append(idx + 1)\n",
    "\n",
    "df[\"start_positions\"] = start_positions\n",
    "df[\"end_positions\"] = end_positions\n",
    "\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "data = {'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'start_positions':start_positions,\n",
    "        'end_positions': end_positions,\n",
    "       }\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('encoding_train.csv',index=False)\n",
    "train = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53b6d71",
   "metadata": {},
   "source": [
    "Initialize Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a75e9ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "# Convert the dataset to a dictionary\n",
    "data_dict = bio_asq[\"test\"].to_dict()\n",
    "# Create a DataFrame from the dictionary\n",
    "df = pd.DataFrame.from_dict(data_dict)\n",
    "\n",
    "questions = [q.strip() for q in df[\"question\"]]\n",
    "context = [q.strip() for q in df[\"context\"]]\n",
    "inputs = tokenizer(\n",
    "        questions,\n",
    "        context,\n",
    "        max_length=384,\n",
    "        truncation=\"only_second\",\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "\n",
    "start_positions = []\n",
    "end_positions = []\n",
    "answers = df['answers']\n",
    "for i, offset in enumerate(offset_mapping):\n",
    "    answer = answers[i]\n",
    "    start_char = answer[0][\"answer_start\"]\n",
    "    end_char = answer[0][\"answer_start\"] + len(answer[0][\"text\"])\n",
    "    sequence_ids = inputs.sequence_ids(i)\n",
    "\n",
    "    # Find the start and end of the context\n",
    "    idx = 0\n",
    "    while sequence_ids[idx] != 1:\n",
    "        idx += 1\n",
    "    context_start = idx\n",
    "    while sequence_ids[idx] == 1:\n",
    "        idx += 1\n",
    "    context_end = idx - 1\n",
    "\n",
    "    # If the answer is not fully inside the context, label it (0, 0)\n",
    "    if offset[context_start][0] > end_char or offset[context_end][1] < start_char:\n",
    "        start_positions.append(0)\n",
    "        end_positions.append(0)\n",
    "    else:\n",
    "        # Otherwise it's the start and end token positions\n",
    "        idx = context_start\n",
    "        while idx <= context_end and offset[idx][0] <= start_char:\n",
    "            idx += 1\n",
    "        start_positions.append(idx - 1)\n",
    "\n",
    "        idx = context_end\n",
    "        while idx >= context_start and offset[idx][1] >= end_char:\n",
    "            idx -= 1\n",
    "        end_positions.append(idx + 1)\n",
    "\n",
    "df[\"start_positions\"] = start_positions\n",
    "df[\"end_positions\"] = end_positions\n",
    "\n",
    "data = {'input_ids': inputs['input_ids'],\n",
    "        'attention_mask': inputs['attention_mask'],\n",
    "        'start_positions':start_positions,\n",
    "        'end_positions': end_positions,\n",
    "       }\n",
    "df = pd.DataFrame(data)\n",
    "df.to_csv('encoding_test.csv',index=False)\n",
    "test = Dataset.from_pandas(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fc9064b",
   "metadata": {},
   "source": [
    "Initialize Model and Training Args (QA Model is used, we shoudl use a encoder-decoder model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9d6c6c80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting evaluate\n",
      "  Using cached evaluate-0.4.3-py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: datasets>=2.0.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from evaluate) (3.6.0)\n",
      "Requirement already satisfied: numpy>=1.17 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from evaluate) (2.1.2)\n",
      "Requirement already satisfied: dill in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from evaluate) (0.3.8)\n",
      "Requirement already satisfied: pandas in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from evaluate) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.19.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from evaluate) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from evaluate) (4.67.1)\n",
      "Requirement already satisfied: xxhash in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from evaluate) (3.5.0)\n",
      "Requirement already satisfied: multiprocess in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from evaluate) (0.70.16)\n",
      "Requirement already satisfied: fsspec>=2021.05.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (2024.6.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.7.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from evaluate) (0.32.1)\n",
      "Requirement already satisfied: packaging in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from evaluate) (25.0)\n",
      "Requirement already satisfied: filelock in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (3.13.1)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (20.0.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from datasets>=2.0.0->evaluate) (6.0.2)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from fsspec[http]>=2021.05.0->evaluate) (3.12.4)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.4)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\n",
      "Requirement already satisfied: idna>=2.0 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (3.10)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from huggingface-hub>=0.7.0->evaluate) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (3.4.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from requests>=2.19.0->evaluate) (2025.4.26)\n",
      "Requirement already satisfied: colorama in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from tqdm>=4.62.1->evaluate) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from pandas->evaluate) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from pandas->evaluate) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\ai\\diagnoses prediction\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\n",
      "Using cached evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "Installing collected packages: evaluate\n",
      "Successfully installed evaluate-0.4.3\n"
     ]
    }
   ],
   "source": [
    "!pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b3601ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForQuestionAnswering were not initialized from the model checkpoint at dmis-lab/biobert-v1.1 and are newly initialized: ['qa_outputs.bias', 'qa_outputs.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForQuestionAnswering, TrainingArguments, Trainer\n",
    "from transformers import DefaultDataCollator\n",
    "import evaluate\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(\"dmis-lab/biobert-v1.1\")\n",
    "\n",
    "data_collator = DefaultDataCollator()\n",
    "\n",
    "# Your existing code to load model, data collator, etc.\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"qa_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=1e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    report_to=[],\n",
    "    logging_dir=\"./logs\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0905ca9",
   "metadata": {},
   "source": [
    "Compute Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d9f9b59d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(p):\n",
    "    predictions, labels = p.predictions, p.label_ids\n",
    "\n",
    "    # Convert tuples to numpy arrays\n",
    "    predictions = np.array(predictions)\n",
    "    labels = np.array(labels)\n",
    "\n",
    "    # Assuming your model outputs logits and you want to get predictions\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "    labels = labels\n",
    "\n",
    "    # Flatten the predictions and labels\n",
    "    predictions = predictions.flatten()\n",
    "    labels = labels.flatten()\n",
    "\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, average='macro')\n",
    "    recall = recall_score(labels, predictions, average='macro')\n",
    "    f1 = f1_score(labels, predictions, average='macro')\n",
    "\n",
    "    return {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a893c6e",
   "metadata": {},
   "source": [
    "Initialize Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a61e95df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Berat\\AppData\\Local\\Temp\\ipykernel_32364\\1697215081.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='450' max='450' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [450/450 09:04, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.312181</td>\n",
       "      <td>0.395000</td>\n",
       "      <td>0.283300</td>\n",
       "      <td>0.308488</td>\n",
       "      <td>0.267232</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.748984</td>\n",
       "      <td>0.452500</td>\n",
       "      <td>0.381322</td>\n",
       "      <td>0.376882</td>\n",
       "      <td>0.347317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.651917</td>\n",
       "      <td>0.479167</td>\n",
       "      <td>0.406126</td>\n",
       "      <td>0.404670</td>\n",
       "      <td>0.373443</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\Diagnoses Prediction\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\AI\\Diagnoses Prediction\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\AI\\Diagnoses Prediction\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\AI\\Diagnoses Prediction\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\AI\\Diagnoses Prediction\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\AI\\Diagnoses Prediction\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=450, training_loss=2.503788791232639, metrics={'train_runtime': 547.903, 'train_samples_per_second': 13.141, 'train_steps_per_second': 0.821, 'total_flos': 1411002486374400.0, 'train_loss': 2.503788791232639, 'epoch': 3.0})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=test,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaf2c37",
   "metadata": {},
   "source": [
    "Evaluate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f8818be7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\AI\\Diagnoses Prediction\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "d:\\AI\\Diagnoses Prediction\\.venv\\Lib\\site-packages\\sklearn\\metrics\\_classification.py:1565: UndefinedMetricWarning: Recall is ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 1.6519168615341187,\n",
       " 'eval_accuracy': 0.4791666666666667,\n",
       " 'eval_precision': 0.4061263742274839,\n",
       " 'eval_recall': 0.40467048536355466,\n",
       " 'eval_f1': 0.3734429931481171,\n",
       " 'eval_runtime': 16.3682,\n",
       " 'eval_samples_per_second': 36.656,\n",
       " 'eval_steps_per_second': 2.322,\n",
       " 'epoch': 3.0}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = trainer.evaluate()\n",
    "results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
